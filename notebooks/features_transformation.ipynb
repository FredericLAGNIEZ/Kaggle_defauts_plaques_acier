{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importations\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler,StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Import généraux\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate\n",
    "import math\n",
    "\n",
    "\n",
    "#target enconding pour la cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(name):\n",
    "    # Définir le chemin du dossier actuel du notebook\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Accéder au répertoire parent\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    raw_data_dir = parent_dir  + \"/data_to_use/raw\"\n",
    "\n",
    "    # Afficher le chemin du répertoire parent\n",
    "    print(\"Répertoire des données brutes: \\n\", raw_data_dir)\n",
    "    \n",
    "    #Chargement d'un dataframe\n",
    "    df=pd.read_csv(raw_data_dir + name, index_col=0)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def split_data(df,target,test_size=0.2):\n",
    "    \"\"\"\n",
    "    Fonction qui sépare les variables des cibles au sein d'un DataFrame\n",
    "    Args:\n",
    "        test_size (float, optional): _description_. Defaults to 0.2.\n",
    "    \"\"\"\n",
    "    X = df.drop(target,axis =1)\n",
    "    y = df[targets]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charegement et affichage du DataFrame\n",
    "df=load_raw_data(name = \"/train.csv\")\n",
    "\n",
    "#Séparation des variables et des cibles\n",
    "features = df.iloc[:,:-7]\n",
    "print(\"Features : \", features.columns)\n",
    "targets = df.iloc[:,-7:]\n",
    "print(\"Targets : \", targets.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation par les Quantiles\n",
    "\n",
    "def apply_transform_Quantile_encoder(df,features,targets):\n",
    "    # Séparer les colonnes cibles du DataFrame d'origine\n",
    "    df_targets = df[targets]\n",
    "    df_qe = df.drop(labels=targets, axis=1)\n",
    "\n",
    "    # Initialize the QuantileTransformer\n",
    "    quantile_encoder = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "\n",
    "    # Fit and transform the continuous variables, à l'aide d'une boucle for\n",
    "    for feature in features:    \n",
    "        #Appliquer l'encodage à la colonne\n",
    "        encoded_data = quantile_encoder.fit_transform(df_qe[[feature]])\n",
    "        #transformer les tableaux numpy en Dataframe\n",
    "        encoded_data = pd.DataFrame(encoded_data,columns=df_qe[[feature]].columns)\n",
    "\n",
    "        df_qe = pd.concat([df_qe.drop(labels=feature,axis = 1),encoded_data], axis = 1)\n",
    "    df = pd.concat([df_qe,df_targets], axis = 1)\n",
    "    \n",
    "    return df_encoder    \n",
    "\n",
    "def pipeline_transform_quantile():\n",
    "    \n",
    "    # Initialize the QuantileTransformer\n",
    "    quantile_encoder = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "    \n",
    "    # Création d'un pipeline pour la transformation\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('quantile_transform', quantile_encoder, [2, 3, 4, 5, 6, 7, 8, 9, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25]),  # Apply log transformation to those columns\n",
    "            #('robust_scaler', scaler, [2, 3, 4, 5, 6, 7, 8, 9, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25]),  # Apply RobustScaler to those columns\n",
    "            ('standardscaler', sc, [0, 1, 10, 11, 12, 14, 24, 26])  # Apply StandardScaler to those features\n",
    "        ],\n",
    "        remainder='passthrough'  # Passthrough the remaining features\n",
    "    )    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def pipeline_transform_Log_Standard_scaler():\n",
    "    \"\"\"    \n",
    "    Transformation : log_transform (+ robust scaler pour les colonnes avec outliers)\n",
    "    Standardscaler pour les autres colonnes    \n",
    "    \"\"\"\n",
    "    # Définition des transformations\n",
    "    log_transform = FunctionTransformer(np.log1p, validate=True)\n",
    "    scaler = RobustScaler()\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # Création d'un pipeline pour la transformation\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('log_transform', log_transform, [2, 3, 4, 5, 6, 7, 8, 9, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25]),  # Apply log transformation to those columns\n",
    "            #('robust_scaler', scaler, [2, 3, 4, 5, 6, 7, 8, 9, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25]),  # Apply RobustScaler to those columns\n",
    "            ('standardscaler', sc, [0, 1, 10, 11, 12, 14, 24, 26])  # Apply StandardScaler to those features\n",
    "        ],\n",
    "        remainder='passthrough'  # Passthrough the remaining features\n",
    "    )\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuer\n",
    "\n",
    "_train, X_test, y_train, y_test=load_split_data(targets=targets,path_raw_data=path_raw_data)\n",
    "X_train_preprocessed, X_test_preprocessed, y_train, y_test, pipeline = transformation_data_logistic_regression(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = predictions(X_test,y_test,pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plaques_acier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
